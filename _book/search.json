[
  {
    "objectID": "note3.html",
    "href": "note3.html",
    "title": "3  The distribution of quadratic forms (More on degree of freedom)",
    "section": "",
    "text": "This can be derived from (Cochran 1934)\n\nThe number \\(n-p\\) is the rank of form Q, i.e. the smallest number of independent variables on which the form may be brought by a non-singular linear transformation. In statistical applications, this number of free variables entering into a problem is usually, in accordance with the terminology introduced by R.A. Fisher, denoted as the number of degree of freedom of the problem, or of the distribution of the random variables attached to the problem.\n\n\nTheorem 3.1 (Cochran’s theorem) Let \\(X_1, \\cdots, X_N\\) be i.i.d. standard normal, and\n\\[\\mathbf{X}=\\left[\\begin{array}{c}X_1\\\\ \\vdots\\\\X_N\\\\ \\end{array}\\right],\\]\na random vector of random variables.\nLet \\(B^{(1)}, B^{(2)},\\cdots,B^{(k)}\\) be symmetric matrices with rank \\(r_i\\). Then,\n\\[Q_i=U^TB^{(i)}U \\] so that the \\(Q_i\\) are quadratic forms. Further assume\n\\[\\sum_i Q_i= U^TU\\]\nThen, the following are equivalent:\n\n\\(r_1+\\cdots +r_k =N\\)\n\\(Q_i\\) are independent\n\n\nDenote \\(Y \\sim N(\\bf{0}, \\sigma^2 I_n)\\) is\n\nLemma\n\n\nref to zhihu\n\n\n\n\n\nCochran, W. G. 1934. “The Distribution of Quadratic Forms in a Normal System, with Applications to the Analysis of Covariance.” Mathematical Proceedings of the Cambridge Philosophical Society 30 (2): 178–91. https://doi.org/10.1017/S0305004100016595."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Cochran, W. G. 1934. “The Distribution of Quadratic Forms in a\nNormal System, with Applications to the Analysis of Covariance.”\nMathematical Proceedings of the Cambridge Philosophical Society\n30 (2): 178–91. https://doi.org/10.1017/S0305004100016595."
  },
  {
    "objectID": "note1.html#convergence-and-statistics",
    "href": "note1.html#convergence-and-statistics",
    "title": "1  Review on Basic Probabilities, Sampling Distribution and Normal related Distributions",
    "section": "1.1 Convergence and Statistics",
    "text": "1.1 Convergence and Statistics\n\n1.1.1 Expectation\n\nExpectation for Discrete Random Variable: \\[\nE(X)=\\sum x_i f(x_i)\n\\]\n\n\nExpectation for Continuous Random Variable: \\[\nE(X)=\\int x f(x) dx\n\\]\n\n\n\n1.1.2 Types of Convergence\nIn this section, we will develop the theoretical background to study the convergence of a sequence of random variables in more detail. In particular, we will define different types of convergence. When we say that the sequence \\(X_n\\) converges to \\(X\\), it means that \\(X_n\\) ‘s are getting’‘closer and closer’’ to \\(X\\). Different types of convergence refer to different ways of defining what ‘’closer’’ means. We also discuss how different types of convergence are related.\n\n1.1.2.1 Convergence of Sequence\n\nConvergence of Sequence: A sequence \\(a_1,a_2,a_3, \\cdots, a_n\\) converges to a limit \\(L\\) if \\[\n\\lim_{n\\rightarrow \\infty} a_n=L\n\\] That is, for any \\(\\epsilon&gt;0\\), there exists an \\(N\\in \\mathbb{N}\\) such that \\[\n|a_n-L|&lt;\\epsilon, \\quad \\text{ for all } n&gt; N\n\\]\n\n\n\n1.1.2.2 Convergence in Distribution\nConvergence in distribution is in some sense the weakest type of convergence. All it says is that the CDF of \\(X_n\\)’’s converges to the CDF of \\(X\\) as \\(n\\) goes to infinity. It does not require any dependence between the \\(X_n\\)’s and \\(X\\).\n\nConvergence in Distribution: A sequence of random variables \\(X_1,X_2,X_3,\\cdots, X_n,\\cdots\\) convergences in distribution to a random variable \\(X\\), shown by\n\\[\nX_n \\ \\xrightarrow{d}\\ X\n\\] if \\[\n\\lim_{n\\rightarrow \\infty} F_{X_n}(x)=F_X(x)\n\\] for all x at which \\(F_X(x)\\) is continuous.\n\nThe Central Limit Theorem (CLT) is an example of the convergence in distribution.\n\n\n1.1.2.3 Convergence in Probability\n\nConvergence in Probability: A sequence of random variables \\(X_1,X_2,X_3,\\cdots, X_n,\\cdots\\) convergences in probability to a random variable \\(X\\), shown by\n\\[\nX_n\\xrightarrow{P}X\n\\] if \\[\n\\lim_{n \\rightarrow \\infty}P(|X_n-X|\\geq \\epsilon)=0, \\quad \\text{for all } \\epsilon &gt;0\n\\]\n\n\n\n1.1.2.4 Convergence in Mean\n\nConvergence in Mean:\nLet \\(r\\geq 1\\) be a fixed number, a sequence of random variables \\(X_1,X_2,\\cdots\\) converges in the \\(r\\)-th mean or in the \\(L^r\\) norm to a random variable \\(X\\), shown by \\(X_n \\xrightarrow{L^r} X\\), if \\[\n\\lim_{n \\rightarrow \\infty} E(|X_n-X|^r)=0.\n\\] If \\(r=2\\), it is called the mean-square convergence, and it is shown by \\[\nX_n\\xrightarrow{m.s.}X.\n\\]\n\n\n\n1.1.2.5 Convergence Almost Surely\n\nIn general, if the probability that the sequence \\(X_n(s)\\) converges to \\(X(s)\\) is equal to \\(1\\), we say that \\(X_n\\) converges to \\(X\\) almost surely and write \\[\nX_n \\xrightarrow{a.s.} X\n\\] if \\[\nP\\left(\\left\\{s\\in S: \\lim_{n\\rightarrow \\infty}X_n(s)=X(s)\\right\\}\\right)=1\n\\]\n\nIt worth mentioning that, the concepts of convergence in probability and almost sure convergence in probability theory are specialisations of the concepts of convergence in measure and pointwise convergence almost everywhere in measure theory.\n\n\n\n1.1.3 Basic Probability Theory\n\n1.1.3.1 \\(\\sigma\\)-algrbra, algebra and semi-ring\n\nDefinition: algebra, \\(\\sigma\\)-algrbra, semi-ring\n\nFAQ: Example of an algebra not a \\(\\sigma\\)-algebra\nNot required, just as an introduction. If you are very intersted in probability theory, it is a recommendation to find out why we need these definitions.\n\n\n\n1.1.4 Sequence of Random Variables\nIn statistics, we draw a sample to make inference of the population, then, if we repeatly draw samples, we will have a sequence of samples from the same population, we usually refer them as i.i.d. (independent and identical distributed) or random samples. This can be denoted as\n\\[\n\\{\\Omega,\\Sigma,P\\}\n\\] where \\(\\Omega\\) is the sample space,\n\\[\n\\Omega=\\{\\omega_1,\\omega_2,\\cdots, \\omega_n\\}, \\quad w_i \\text{ are simple(single) events}\n\\]\n\\(\\Sigma\\) is the \\(\\sigma\\)-algebra (You may consider it is set of the sets of simple events in brief) and \\(P\\) is a probability measure.\nHowever, if we consider the samples not necessarily from the same population, we may have a sequence of random variables \\(X_1,X_2,\\cdots\\), and an correspnded underlying sample space \\(\\Omega\\). In particular, each \\(X_n\\) is a function from its \\(\\Omega\\), to real numbers through the probability measure \\(P\\).\nIn other words, a sequence of random variables is in fact a sequence of functions (Mapping, or \\(P\\), or a probability measure) \\(X_n:\\Omega\\rightarrow \\mathbb{R}\\) , such as\n\\[\nP(\\omega_i)=x_i, \\quad \\omega_i \\in \\Omega \\text{ and } \\sum x_i=1, \\quad i = 1,\\cdots,n\n\\]\n\n1.1.4.0.0.1 Example: Convergence of Sequence of R.V.\nConsider the following random experiment: A fair coin is tossed once. Here, the sample space has only two elements \\(S=\\{H,T\\}\\). We define a sequence of random variables \\(X_1,X_2,\\cdots\\) on this sample space as follows:\n\\[\n\\nonumber X_n(s) = \\left\\{\n\\begin{array}{l l}\n\\frac{1}{n+1} & \\qquad \\textrm{ if }s=H \\\\\n& \\qquad \\\\\n1 & \\qquad \\textrm{ if }s=T\n\\end{array} \\right.\n\\]\n\nAre the \\(X_i\\) independent?\nNo, they are dependent as they are measuring the same coin.\nFind the PMF and CDF of \\(X_n\\), \\(F_{X_n}(x)\\) for \\(n=1,2,3,\\cdots\\).\nThe PMF are\n\n\\[\n   \\nonumber P_{{\\large X_n}}(x)=P(X_n=x) = \\left\\{\n   \\begin{array}{l l}\n   \\frac{1}{2} & \\qquad \\textrm{ if }x=\\frac{1}{n+1} \\\\\n   & \\qquad \\\\\n   \\frac{1}{2} & \\qquad \\textrm{ if }x=1\n   \\end{array} \\right.\n\\]\nCorrespondingly, the CDF are\n\\[\n   \\nonumber F_{{\\large X_n}}(x)=P(X_n \\leq x) = \\left\\{\n   \\begin{array}{l l}\n   1 & \\qquad \\textrm{ if }x \\geq 1\\\\\n   & \\qquad \\\\\n   \\frac{1}{2} & \\qquad \\textrm{ if }\\frac{1}{n+1} \\leq x &lt;1 \\\\\n   & \\qquad \\\\\n   0 & \\qquad \\textrm{ if }x&lt; \\frac{1}{n+1}\n   \\end{array} \\right.\n\\]\n\nAs \\(n\\) goes to infinity, what does \\(F_{X_n}(x)\\) look like?"
  },
  {
    "objectID": "note1.html#sampling-distribution",
    "href": "note1.html#sampling-distribution",
    "title": "1  Review on Basic Probabilities, Sampling Distribution and Normal related Distributions",
    "section": "1.2 Sampling Distribution",
    "text": "1.2 Sampling Distribution\n\n1.2.1 Statistic and Parameter\n\nDefinition: Statistics\nA statistic is a function of the observable random variables in a sample and known constants.\n\n\n\n\n\nStatistic\nParameter\n\n\n\n\nMean\n\\(\\bar{x}\\)\n\\(\\mu\\)\n\n\nStandard Deviation\n\\(s\\)\n\\(\\sigma\\)\n\n\nProportion\n\\(\\hat{p}\\)\n\\(p\\)\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\nIn general\n\\(\\hat{\\theta}\\)\n\\(\\theta\\)\n\n\n\n\nDefinition: Sampling Distribution.\nAll statistics have probability distributions, which we will call them sampling distributions.\n\n\n\n1.2.2 Sample Mean\n\nSample Mean: \\[\n\\bar{X}=\\frac{\\sum_{i=1}^n X_i}{n}\n\\] refered as a random varible \\(\\bar{X}\\) as a function of random variables \\(X_1,X_2,\\cdots, X_n\\).\n\nHere we discuss the mean and variance of this random variable, that is to say the R.V. \\(\\bar{X}\\).\n\n\n1.2.3 Law of Large Numbers\nTo start, we specify the Law of large numbers. There are two main versions of the Law of large numbers\nThe Weak Law of Large Numbers (WLLN):\nLet \\(X_1,X_2,\\cdots,X_n\\) be i.i.d. random variables with finite expected value \\(E(X_i)=\\mu&lt;\\infty\\). Then, for any \\(\\epsilon&gt;0\\),\n\\[\\lim_{n\\rightarrow \\infty}P\\big(|\\bar{X}-\\mu| \\geq \\epsilon\\big)=0 \\]\nThat is\n\\[\n\\bar{X}\\xrightarrow{p} \\mu\n\\]\nProof\nAssume the variance of \\(X\\) be \\(Var(X)=\\sigma^2\\) is finite. In this case, we can use Chebyshev’s inequality to write \\[\nP(|\\bar{X}-\\mu| \\geq \\epsilon) \\leq \\frac{Var(\\bar{X})}{\\epsilon^2}=\\frac{Var(X)}{n\\epsilon^2}\n\\] This goes to zero as \\(n\\rightarrow \\infty\\).\n\nThe Strong Law of Large Numbers (SLLN): Let \\(X_1,X_2,\\cdots,X_n\\) be i.i.d. random variables with finite expected value \\(E(X_i)=\\mu&lt;\\infty\\).\n\n\\[\n\\bar{X} \\xrightarrow{a.s.} \\mu\n\\]"
  },
  {
    "objectID": "note1.html#reference",
    "href": "note1.html#reference",
    "title": "1  Review on Basic Probabilities, Sampling Distribution and Normal related Distributions",
    "section": "1.3 Reference",
    "text": "1.3 Reference\n\nPishro-Nik, H. (2016). Introduction to probability, statistics, and random processes.\nWackerly, D., Mendenhall, W., & Scheaffer, R. L. (2014). Mathematical statistics with applications. Cengage Learning.\nCasella, G., & Berger, R. L. (2021). Statistical inference. Cengage Learning.\nHogg, R. V., & Craig, A. T. (1995). Introduction to mathematical statistics.(5”” edition). Englewood Hills, New Jersey.\nTerry Tao’s Blog\nt-distribution"
  },
  {
    "objectID": "note2.html#normal-distribution",
    "href": "note2.html#normal-distribution",
    "title": "2  Normal Distribution, Chi-Square Distribution, t Distribution and F Distribution",
    "section": "2.1 Normal Distribution",
    "text": "2.1 Normal Distribution\n\nNormal Distribution:\n\n\\[\nf_X(x)=\\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{1}{2\\sigma^2}(x-\\mu)^2}\n\\]\n\nTheorem: Let \\(Y_1, Y_2, \\cdots, Y_n\\) be a random sample of size \\(n\\) from a normal distribution with mean \\(\\mu\\) and variance \\(\\sigma^2\\). Then,\n\n\\[\n\\bar{Y}=\\frac{1}{n}\\sum_{i=1}^n Y_i\n\\] is normally distributed with mean \\(\\mu_{\\bar{Y}}=E(\\bar{Y})=\\mu\\) and variance \\(\\sigma^2_{\\bar{Y}}=\\frac{\\sigma^2}{n}\\)\n\n2.1.0.1 Standardized Z-Value\n\nStandard Normal Distribution: Z \\[\nZ=\\frac{\\bar{Y}-\\mu_{y}}{\\sigma_{\\bar{Y}}}=\\frac{\\bar{Y}-\\mu}{\\sigma/\\sqrt{n}}\n\\]\n\nWe can use the Standard Normal Table or the softwares (R) to find the corresponding quantiles and probability.\n\nqnorm(p,mu,sigma)\n\npnorm(q,mu,sigma)\n\n\n2.1.0.1.0.1 Example: Calculation\nA bottling machine can be regulated so that it discharges an average of \\(\\mu\\) ounces perbottle. It has been observed that the amount of fill dispensed bythe machine is normally distributed with \\(\\sigma = 1.0\\) ounce. A sample of \\(n = 9\\) filled bottles is randomly selected from the output of the machine on a given day (all bottled with the same machine setting), and the ounces of fill are measured for each.\n\nFind the probability that the sample mean will be within \\(0.3\\) ounce of the true mean \\(\\mu\\) for the chosen machine setting.\nHow many observations should be included in the sample if we wish \\(Y\\) to be within .3 ounce of \\(\\mu\\) with probability .95?"
  },
  {
    "objectID": "note2.html#the-chi-square-distribution-a-special-gamma-distribution",
    "href": "note2.html#the-chi-square-distribution-a-special-gamma-distribution",
    "title": "2  Normal Distribution, Chi-Square Distribution, t Distribution and F Distribution",
    "section": "2.2 The Chi-Square distribution (a special Gamma distribution)",
    "text": "2.2 The Chi-Square distribution (a special Gamma distribution)\nTheorem: Let \\(Y_1, Y_2, \\cdots, Y_n\\) be a random sample of size \\(n\\) from a normal distribution with mean \\(\\mu\\) and variance \\(\\sigma^2\\).\nThen,\n\\[\nZ_i=\\frac{Y_i-\\mu}{\\sigma}\n\\]\nare independent standard normal random variables, \\(i=1,\\cdots,n\\) and\n\\[\n\\sum_{i=1}^n Z_i^2=\\sum_{i}\\left(\\frac{Y_i-\\mu}{n}\\right)^2\n\\]\nhas a \\(\\chi^2\\)-distribution with \\(n\\) degree of freedom (df).\nThe proof can be conducted by the moment generating function.\n\n2.2.0.0.1 Example: \\(\\chi^2\\) distribution derviation\nThe moment generating function of \\(Z^2\\) is\n\\[\n\\begin{aligned}\nm_{Z^2}(t)=E(e^{tZ^2})&=\\int_{-\\infty}^\\infty e^{tz^2}f(z)dz\\\\\n&= \\int_{-\\infty}^{\\infty} \\frac{1}{\\sqrt{2\\pi}} e^{-z^2(1-2t)/2}dz\\\\\n&=\\frac{1}{(1-2t)^{1/2}} \\int\\frac{1}{\\sqrt{2\\pi}(1-2t)^{-1/2}}\\exp\\left(-\\frac{z^2/2}{(1-2t)^{-1}}\\right)dz \\\\\n&=\\frac{1}{(1-2t)^{1/2}} \\\\\n\\end{aligned}\n\\]\nwith the corresponding density function of \\(U=Z^2\\) is given by\n\\[\nf_U(u)=\\frac{u^{-1/2}e^{-u/2}}{\\Gamma(1/2)2^{1/2}}\n\\]\nwhich, is a Gamma distribution with (\\(\\alpha=1/2, \\beta=2\\)) and is also called the \\(\\chi^2\\) distribution with degree of freedom 1.\nMore over, for the \\(V=\\sum_{i=1}^n Z_i^2=\\sum_{i=1}^nU_i\\),\n\\[\nm_V(t)=\\prod(m_{Z_i^2}(t))=\\left((1-2t)^{-1/2}\\right)^n=(1-2t)^{-n/2}\n\\]\nwith the corresponding density function,\n\\[\nf_V(v)=\\frac{u^{-n/2}e^{-u/2}}{\\Gamma(n/2)2^{n/2}}\n\\]\nwhich is a Gamma(\\(\\alpha\\), \\(\\beta\\)) and \\(\\alpha=n/2\\) and \\(\\beta=2\\),\n\\[\nf(x)=\\frac{1}{\\Gamma(\\alpha)\\beta^\\alpha}x^{\\alpha-1}e^{-x/\\beta}\n\\]\n\n\n2.2.0.0.2 Example: Calculation\nIf \\(Z_1,\\cdots,Z_6\\) denotes a random sample from standard normal distribution, find a number b such that \\[\nP(\\sum_{i=1}^6Z^2_i\\leq b)=0.95\n\\]\nAs \\(\\sum_{i=1}^6 Z^2_i\\) has a \\(\\chi^2\\) distribution with degree of freedom 6, we can get either in the \\(\\chi^2\\) table or using the software, such as rcode below\n\nqchisq(0.95,6)\n\n[1] 12.59159\n\n\nFind out that\n\\[\nP(\\sum_{i=1}^6Z^2_i\\leq 12.59159)=0.95\n\\]\n\nTheorem: Let \\(Y_1, Y_2, \\cdots, Y_n\\) be a random sample of size \\(n\\) from a normal distribution with mean \\(\\mu\\) and variance \\(\\sigma^2\\). Then, \\[\n\\frac{(n-1)s^2}{\\sigma^2}=\\frac{\\sum_{i=1}^n(Y_i-\\bar{Y})^2}{\\sigma^2}\n\\] has a \\(\\chi^2\\) distribution with \\(n-1\\) degree of freedom.\n\\(\\bar{Y}\\) and \\(S^2\\) are independent random variables.\n\n\n\n2.2.0.0.3 Example: Prove the above theorem when \\(n=2\\)"
  },
  {
    "objectID": "note2.html#the-t-distribution",
    "href": "note2.html#the-t-distribution",
    "title": "2  Normal Distribution, Chi-Square Distribution, t Distribution and F Distribution",
    "section": "2.3 The t distribution",
    "text": "2.3 The t distribution\n\nTheorem: Let \\(Z\\) be a standard normal random variable and let \\(W\\) be a \\(\\chi^2\\) distributed variable with \\(k\\) degree of freedom. Then, if \\(Z\\) and \\(W\\) are independent,\n\n\\[\nT=\\frac{Z}{\\sqrt{W/k}}\n\\]\nis a t distribution with d.f. \\(k\\), with density function,\n\\[\nf(t)=\\frac{\\Gamma(\\frac{k+1}{2})}{\\Gamma(\\frac{k}{2})}\\,\n\\frac{1}{\\sqrt{k\\,\\pi}}\\,\n\\left(1+\\frac{t^2}{k}\\right)^{-\\frac{k+1}{2}}\n\\]\n\n2.3.0.0.1 Example: Suppose that \\(T\\) is R.V. as above, derive its p.d.f.\n\nIf \\(T\\) is given by \\(\\frac{U}{\\sqrt{V/k}}\\), find the joint density of \\(U\\) and \\(V\\).\nFind the density function of \\(T\\).\n\n\\[\n   f_{U,V}(u,v) = \\underbrace{\\frac{1}{(2\\pi)^{1/2}} e^{-u^2/2}}_{\\text{pdf } N(0,1)}\\quad \\underbrace{\\frac{1}{\\Gamma(\\frac{k}{2})\\,2^{k/2}}\\,v^{(k/2)-1}\\, e^{-v/2}}_{\\text{pdf }\\chi^2_k}\n\\]\nDenote\n\\[\n   t=\\frac{u}{\\sqrt{v/k}}, \\quad w=v\n\\]\nwhere\n\\[\n   u=t(\\frac{w}{k})^{1/2}, \\quad v= w\n\\]\nThe Jacobian matrix can be find as\n\\[\n   J=\\begin{vmatrix}\n   \\frac{du}{dt} & \\frac{du}{dw}\\\\\n   \\frac{dv}{dt} & \\frac{dv}{dw}\\\\\n   \\end{vmatrix}=\\begin{vmatrix}\n   (\\frac{w}{k})^{1/2} & \\frac{1}{2}t(\\frac{1}{wk})^{1/2}\\\\\n   0&1\n   \\end{vmatrix}=(\\frac{w}{k})^{1/2}\n\\]\nHence, the marginal p.d.f. is\n\\[\n   \\begin{aligned}\n   f_T(t) &= \\displaystyle\\int_0^\\infty \\,f_{U,V}\\bigg(t\\,(\\frac{w}{k})^{1/2},w\\bigg)(w/k)^{1/2}\\,\\mathrm{d} w\\\\[2ex]\n   &= \\frac{1}{(2\\pi)^{1/2}}\\frac{1}{\\Gamma(\\frac{k}{2})2^{k/2}}\\,\n   \\int_0^\\infty\\,\n   e^{-\\frac{\\left(t(\\frac{w}{k})^{1/2}\\right)^2}{2}}\n   w^{(k/2)-1}\n   e^{-(\\frac{w}{2})}\n   \\frac{w^{1/2}}{k^{1/2}}\\,\\mathrm{d}w\\\\[2ex]\n   &= \\frac{1}{(2\\pi)^{1/2}}\\frac{1}{\\Gamma(\\frac{k}{2})2^{k/2}k^{1/2}}\\,\n   \\displaystyle\\int_0^\\infty\\,\n   w^{((k+1)/2)-1}\\,e^{-(1/2)(1 + t^2/k)w}\\,\\mathrm{d}w\n   \\end{aligned}\n\\]\nwhere\n\\[\n   \\int_0^\\infty\\,\n   w^{((k+1)/2)-1}\\,e^{-(1/2)(1 + t^2/k)w}\\,\\mathrm{d}w=\\int_0^{\\infty}w^{\\alpha-1}\\,e^{-\\lambda w}dw=\\frac{\\lambda^\\alpha}{\\Gamma(\\alpha)}\n\\]\nwhere\n\\[\n   \\alpha=(k+1)/2,\\,\\quad \\lambda=(1/2)(1+t^2/k)\n\\]\nThus,\n\\[\n   \\begin{aligned}\n   f_T(t)&= \\frac{1}{(2\\pi)^{1/2}}\\frac{1}{\\Gamma\\left(\\frac{k}{2}\\right)2^{k/2}k^{1/2}}\\,\n   \\frac{\\Gamma\\left((k+1)/2\\right)}{\\left((1/2)(1+t^2/k)\\right)^{(k+1)/2}}\\\\[2ex]\n   &=\\frac{1}{(2\\pi)^{1/2}}\\frac{1}{\\Gamma\\left(\\frac{k}{2}\\right)\\,2^{k/2}k^{1/2}}\\,\\Gamma\\left((k+1)/2\\right)\\,\n   \\Big[\\frac{2}{(1+t^2/k)}\\Big]^{(k+1)/2}\\\\[2ex]\n   &= \\frac{\\Gamma\\left(\\frac{k+1}{2}\\right)}{\\Gamma\\left(\\frac{k}{2}\\right)}\\,\n   \\frac{1}{(2\\pi)^{1/2}2^{k/2}k^{1/2}}\\,\n   \\Big[\\frac{2}{(1+t^2/k)}\\Big]^{(k+1)/2}\\\\[2ex]\n   &=\\frac{\\Gamma\\left(\\frac{k+1}{2}\\right)}{\\Gamma\\left(\\frac{k}{2}\\right)}\\,\n   \\frac{1}{(2\\pi)^{1/2}2^{k/2}k^{1/2}}\\,\n   \\frac{2^{(k+1)/2}}{(1+t^2/k)^{(k+1)/2}}\\\\[2ex]\n   &= \\frac{\\Gamma\\left(\\frac{k+1}{2}\\right)}{\\Gamma\\left(\\frac{k}{2}\\right)}\\,\n   \\frac{1}{(\\pi)^{1/2}k^{1/2}}\\,\n   \\frac{1}{(1+t^2/k)^{(k+1)/2}}\\\\[2ex]\n   &=\\frac{\\Gamma(\\frac{k+1}{2})}{\\Gamma(\\frac{k}{2})}\\,\n   \\frac{1}{\\sqrt{k\\,\\pi}}\\,\n   \\left(1+\\frac{t^2}{k}\\right)^{-\\frac{k+1}{2}}\n   \\end{aligned}\n\\]\nwhich is the pdf of the t-Student or Gosset distribution with \\(k\\) degrees of freedom (or \\(n\\) degrees of freedom).\n\n2.3.0.0.1.1 Example: Calculation\nThe tensile strength for a type of wire is normally distributed with unknown mean \\(\\mu\\) and unknown variance \\(\\sigma^2\\). Six pieces of wire were randomly selected from a large roll; \\(Y_i\\), the tensile strength for portion \\(i\\), is measured for \\(i = 1, 2, . . . , 6\\). The population mean \\(\\mu\\) and variance \\(\\sigma^2\\) can be estimated by \\(\\bar{Y}\\) and \\(s^2\\), respectively.\nFind the approximate probability that \\(\\bar{Y}\\) will be within \\(2S/\\sqrt{n}\\) of the true population mean \\(\\mu\\).\n\npt(2,5)-pt(-2,5)\n\n[1] 0.8980605\n\n\nAs\n\\[\nT=\\frac{\\bar{Y}-\\mu}{S/\\sqrt{n}}\n\\]\nThen\n\\[\nP(|\\bar{Y}-\\mu|\\leq 2S/\\sqrt{n})=P(-2\\leq T \\leq 2)= P(T\\leq 2)-P(T\\leq -2)=?\n\\]"
  },
  {
    "objectID": "note2.html#the-f-distribution",
    "href": "note2.html#the-f-distribution",
    "title": "2  Normal Distribution, Chi-Square Distribution, t Distribution and F Distribution",
    "section": "2.4 The F Distribution",
    "text": "2.4 The F Distribution\nSuppose that we want to compare the variances of two normal populations based on information contained in independent random samples from the two populations.\n\nThe F Distribution: Let \\(W_1\\) and \\(W_2\\) be independent \\(\\chi^2\\) distributed random variables with \\(v_1\\) and \\(v_2\\) degree of freedom. Then, \\[\nF=\\frac{W_1/v_1}{W_2/v_2}=\\frac{(n-1)S^2_1/\\sigma^2_1/(n_1-1)}{(n-1)S^2_2/\\sigma^2_2/(n_2-1)}=\\frac{S^2_1/\\sigma^2_1}{S^2_2/\\sigma^2_2}\n\\] is an F distribution, \\(F(v_1=n_1-1,v_2=n_2-1)\\).\n\n\n2.4.0.0.0.1 Example: Calculation\nIf there are two popluation with equal variance, we draw two sample with size \\(n_1=6\\) and \\(n_2=10\\), such that\n\\[\nP(\\frac{S^2_1}{S^2_2} \\leq b)=0.95\n\\]\nWhat is the value of b?\n\nqf(p,v1,v2)\npf(q,v1,v2)"
  }
]